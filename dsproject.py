# -*- coding: utf-8 -*-
"""DSproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12qNOwFJyQj8iIRwNgK7gx_znIwayJwNC

# Import Relevant Libraries
"""

# !pip install tensorflow
# !pip install keras

import time
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.metrics import auc, f1_score, recall_score, precision_score, accuracy_score, confusion_matrix, log_loss, mean_squared_error, ConfusionMatrixDisplay, roc_curve

from keras import models
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop,Adam
from keras.utils import to_categorical, plot_model

"""# Preprocessing"""

from google.colab import drive
drive.mount('/content/drive')

# Upload the data file
face_df = pd.read_csv('/content/drive/MyDrive/DS4400 Project/icml_face_data.csv')
face_df.head()

# Split the data into training and testing (based on 'Usage' column)
train_df = face_df[face_df[' Usage']=='Training']
test_df = face_df[(face_df[' Usage']=='PublicTest') | (face_df[' Usage']=='PrivateTest')]

# Remove the 'Usage' column ... it is no longer needed
train_df = train_df.drop(columns=[' Usage'])
test_df = test_df.drop(columns=[' Usage'])

test_df.head()

# Split the data into X and Y
X_train = train_df[' pixels']
Y_train = train_df['emotion']
X_test = test_df[' pixels']
Y_test = test_df['emotion']

# Reset indices
X_train.reset_index(inplace=True, drop=True)
Y_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
Y_test.reset_index(inplace=True, drop=True)

# Represent the features as a vector with 2304 numerical values (1D array)

# Split the pixel column so that it is represented by an array rather than a string
X_train = X_train.str.split()
X_test = X_test.str.split()

# Flatten the lists within the Series
X_train = X_train.apply(pd.Series).values
X_test = X_test.apply(pd.Series).values

# Convert pixel values to floats
X_train = X_train.astype(np.float32)
X_test = X_test.astype(np.float32)

# normalize
X_train /= 255
X_test /= 255

# Reshape the 1D array into a 48x48 image
X_train_2D = X_train.reshape((28709, 48, 48))
X_test_2D = X_test.reshape((7178, 48, 48))

# Change (encode) labels into binary categories
Y_train = to_categorical(Y_train, num_classes=7)
Y_test = to_categorical(Y_test, num_classes=7)

# Create a dictionary to store text labels for each numerical (binary vector) label
emotions_key= {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}

print(X_train_2D.shape)
print(X_test_2D.shape)
print(Y_train.shape)
print(Y_test.shape)

"""# Data Exploration"""

# View a sample of the test images
plt.figure(figsize=(10,10))
random_inds = np.random.choice(28709,25)
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    image_ind = random_inds[i]
    plt.imshow(np.squeeze(X_train_2D[image_ind]), cmap=plt.cm.binary_r) # Inverse of images...
    plt.xlabel(emotions_key[np.argmax(Y_train[image_ind])]) # display emotions instead of integers

# Citation: https://github.com/aamini/introtodeeplearning/blob/master/lab2/Part1_MNIST.ipynb

# function to plot frequency counts of labels for given data
def plot_frequencies(Y, title):

  # create an empty array to store emotion labels
  labels = []

  # loop through Y array and append the correct emotion label
  for i in Y:
    labels.append(emotions_key[np.argmax(i)])

  # create a dataframe w/ labels
  labels_df = pd.DataFrame({'Labels': labels})

  # calculate frequency counts for each emotion
  label_counts = labels_df['Labels'].value_counts()
  # plot
  plt.figure(figsize=(10, 6))
  label_counts.plot(kind='bar')
  plt.title(title)
  plt.xlabel('Labels')
  plt.ylabel('Frequency')
  plt.xticks(rotation=45)  # Rotate labels for better readability if needed
  plt.show()

plot_frequencies(Y_train, 'Frequency Count of Labels: Training Set')
plot_frequencies(Y_test, 'Frequency Count of Labels: Testing Set')

"""# Model 1: kNN with Cross-Validation"""

k_vals = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
k_accuracy = {}
k_precision = {}
k_recall = {}
k_error = {}
for k in k_vals:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn = knn.fit(X_train, Y_train)
    knn_pred = knn.predict(X_test)
    k_accuracy[k] = round(accuracy_score(knn_pred, Y_test), 4)
    k_precision[k] = round(precision_score(knn_pred, Y_test, average="macro"), 4)
    k_recall[k] = round(recall_score(knn_pred, Y_test, average="macro"), 4)
    k_error[k] = 1 - round(accuracy_score(knn_pred, Y_test), 4)
    k_cv = cross_val_score(knn, X_test, Y_test, cv=10)


plt.figure(figsize=(10, 5))

#accuracy
plt.subplot(2, 2, 1)
plt.plot(k_vals, list(k_accuracy.values()), marker='o')
plt.title('Accuracy vs. k')
plt.xlabel('k')
plt.ylabel('Accuracy')
plt.xticks(k_vals)
plt.grid(True)

#precision
plt.subplot(2, 2, 2)
plt.plot(k_vals, list(k_precision.values()), marker='o')
plt.title('Precision vs. k')
plt.xlabel('k')
plt.ylabel('Precision')
plt.xticks(k_vals)
plt.grid(True)

#recall
plt.subplot(2, 2, 3)
plt.plot(k_vals, list(k_recall.values()), marker='o')
plt.title('Recall vs. k')
plt.xlabel('k')
plt.ylabel('Recall')
plt.xticks(k_vals)
plt.grid(True)

#error
plt.subplot(2, 2, 4)
plt.plot(k_vals, list(k_error.values()), marker='o')
plt.title('Error vs. k')
plt.xlabel('k')
plt.ylabel('Error')
plt.xticks(k_vals)
plt.grid(True)

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, Y_train)
knn_pred = knn.predict(X_test)

fprs = {}
tprs = {}
aucs = {}
plt.figure()
labels = ['Angry','Disgust','Fear','Happy','Surprise', 'Sad','Neutral']
knn_pred = pd.DataFrame(knn_pred)
knn_pred.shape

Y_test_df = pd.DataFrame(Y_test)
for i in range(7):
    fpr, tpr, t = roc_curve(knn_pred.loc[:, i], Y_test_df.loc[:, i])
    auc_score = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{labels[i]} : AUC = {auc_score:.2f})')


plt.plot([0, 1], [0, 1])  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for KNN')
plt.legend(loc='lower right')
plt.show()
#roc_curve(y_pred.loc[:, 0], y_test.loc[:, 0])

plt.figure(figsize=(10, 7))

#cv score
plt.subplot(2, 2, 1)
plt.plot(k_vals, list(k_cv), marker='o')
plt.title('CV score vs. k')
plt.xlabel('k')
plt.ylabel('CV Score')
plt.xticks(k_vals)
plt.grid(True)

"""# Model 2: Logistic Regression"""

max_iters = 1000
Y_train_max = np.argmax(Y_train, axis=1)
Y_test_max = np.argmax(Y_test, axis=1)

lr = LogisticRegression(multi_class='multinomial', max_iter=1000)

lr.fit(X_train, Y_train_max)
y_pred = lr.predict(X_test)

print(accuracy_score(Y_test_max, y_pred))
print(cross_val_score(lr, X_train, Y_train_max, cv=5, scoring='accuracy'))

Y_test_df = pd.DataFrame(Y_test_max)
y_pred = pd.DataFrame(y_pred)

fprs = {}
tprs = {}
aucs = {}
plt.figure()
labels = ['Angry','Disgust','Fear','Happy','Surprise', 'Sad','Neutral']
for i in range(len(y_pred.columns)):
    fpr, tpr, t = roc_curve(y_pred.loc[:, i], Y_test_df.loc[:, i])
    auc_score = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{labels[i]} : AUC = {auc_score:.2f})')


plt.plot([0, 1], [0, 1])  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Logistic Regression with Softmax')
plt.legend(loc='lower right')
plt.show()
roc_curve(y_pred.loc[:, 0], Y_test_df.loc[:, 0])

"""# Model 3: Feed Forward Neural Network"""

# Initialize the model
MLP_model = models.Sequential()

# Flatten the input data to be used in Dense layers
MLP_model.add(Flatten(input_shape=(48, 48, 1)))

# Define the first Dense layer
MLP_model.add(Dense(128, activation='relu'))

# Define the second Dense layer
MLP_model.add(Dense(128, activation='relu'))

# Define the last Dense layer to output the classification probabilities (7 classes, softmax)
MLP_model.add(Dense(7, activation='softmax'))

# Compile the model with an optimizer, and specify loss type and metrics
MLP_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

# Display the model's architecture
MLP_model.summary()

# Train the model
history = MLP_model.fit(X_train_2D, Y_train, epochs=12, batch_size=64)

# Visualize the model's training loss and accuracy over time
vis_loss = history.history['loss']
vis_acc = history.history['accuracy']
vis_epochs = range(len(vis_acc))

plt.plot(vis_epochs, vis_loss, label='Training Loss')
plt.title('Model Loss over Time')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.plot(vis_epochs, vis_acc, label='Training Accuracy')
plt.title('Model Accuracy over Time')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Generate test predictions
train_pred = MLP_model.predict(X_train_2D)
test_pred = MLP_model.predict(X_test_2D)

# Evaluate the model
train_loss, train_acc = MLP_model.evaluate(X_train_2D, Y_train_2D)
test_loss, test_acc = MLP_model.evaluate(X_test_2D, Y_test_2D)
print('Train loss:', round(train_loss, 3))
print('Train accuracy:', round(train_acc, 3))
print('Test loss:', round(test_loss, 3))
print('Test accuracy:', round(test_acc, 3))

# Confusion matrix for predicted vs. true label (test set)
ConfusionMatrixDisplay.from_predictions(Y_test.argmax(axis=1),
                                        test_pred.argmax(axis=1),
                                        display_labels=emotions_key.values(),
                                        normalize='true',
                                        xticks_rotation='vertical',
                                        cmap='BuPu')
plt.show()

"""# Model 4: Convolutional Neural Network"""

CNN_model = models.Sequential()

# First convolutional layer,
CNN_model.add(Conv2D(filters=32, kernel_size=3, strides=1, activation='relu', input_shape=(48, 48, 1)))

# First pool layer (2x2, stride 2)
CNN_model.add(MaxPool2D(pool_size=2))

# Second convolutional layer
CNN_model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))

# Second pool layer (2x2, stride 2)
CNN_model.add(MaxPool2D(pool_size=2))

# third convolutional layer
CNN_model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))

# Flatten data to be used for dense layer
CNN_model.add(Flatten())

# Define the first Dense layer
CNN_model.add(Dense(64, activation='relu'))

# Define the last Dense layer to output the classification probabilities (7 classes, softmax)
CNN_model.add(Dense(7, activation='softmax'))

# compile model with optimizer... calculate loss and accuracy
CNN_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

CNN_model.summary()

plot_model(CNN_model)

history = CNN_model.fit(X_train_2D, Y_train, epochs=12, batch_size=64)

# Visualize model's change in training loss & accuracy over time

vis_loss = history.history['loss']
vis_acc = history.history['accuracy']

vis_epochs = range(len(vis_acc))

plt.plot(vis_epochs, vis_loss)
plt.title('Model Loss over Time')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.figure()

plt.plot(vis_epochs,vis_acc)
plt.title('Model Accuracy over Time')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.figure()

# generate test predictions
train_pred = CNN_model.predict(X_train_2D)
test_pred = CNN_model.predict(X_test_2D)

# evaluate
train_loss, train_acc = CNN_model.evaluate(X_train_2D, Y_train)
test_loss, test_acc = CNN_model.evaluate(X_test_2D, Y_test)
print('train loss:', round(train_loss,3))
print('train accuracy:', round(train_acc,3))
print('test loss:', round(test_loss,3))
print('test accuracy:', round(test_acc,3))

# Confusion matrix for predicted vs. true label (test set)
ConfusionMatrixDisplay.from_predictions(Y_test.argmax(axis=1),
                                        test_pred.argmax(axis=1),
                                        display_labels=emotions_key.values(),
                                        normalize='true',
                                        xticks_rotation='vertical',
                                        cmap='BuPu')